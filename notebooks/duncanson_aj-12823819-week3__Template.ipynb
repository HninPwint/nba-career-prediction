{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_label = 'zz_template_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_label = 'aj'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aim**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Findings**\n",
    "\n",
    "Findings for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-89cf590f1a1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNBARawData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/src/common_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPolynomialFeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNBARawData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "#Initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.common_lib import DataReader, NBARawData\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data input and cleansing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-379b82d317d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load dataset using common function DataReader.read_data()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load Raw Train Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNBARawData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load Test Raw Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataReader' is not defined"
     ]
    }
   ],
   "source": [
    "#Load dataset using common function DataReader.read_data()\n",
    "data_reader = DataReader()\n",
    "# Load Raw Train Data\n",
    "df_train = data_reader.read_data(NBARawData.TRAIN)\n",
    "# Load Test Raw Data\n",
    "df_test = data_reader.read_data(NBARawData.TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For train dataframe, remove redundant column 'Id_old'\n",
    "### Removing done in Data Split function\n",
    "# cols_drop = [\"Id\", \"Id_old\"]\n",
    "# df_train.drop(cols_drop, axis=1, inplace=True)\n",
    "\n",
    "### Column Strip is already done in Data Reader for Train and Test set, so comment out\n",
    "# df_train.columns = df_train.columns.str.strip()\n",
    "df_train.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For test dataframe, remove redundant column 'Id_old'\n",
    "# df_test.drop(cols_drop, axis=1, inplace=True)\n",
    "# df_test.columns = df_test.columns.str.strip()\n",
    "# df_test.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Negative values in dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.where(df_train < 0).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative values do not make sense in this context\n",
    "#Define negative cleaning function\n",
    "def clean_negatives(strategy, df):\n",
    "    \n",
    "    if strategy=='abs':\n",
    "        df = abs(df)\n",
    "    if strategy=='null':\n",
    "        df[df < 0] = None\n",
    "    if strategy=='mean':\n",
    "        df[df < 0] = None\n",
    "        df.fillna(df.mean(), inplace=True)      \n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean negative numbers\n",
    "negatives_strategy = 'abs'\n",
    "\n",
    "df_train = clean_negatives(negatives_strategy, df_train)\n",
    "df_test = clean_negatives(negatives_strategy, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Correlation and Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Pearson Correlation to determine feature correlation \n",
    "pearsoncorr = df_train.corr('pearson')\n",
    "\n",
    "#Create heatmap of pearson correlation factors\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sb.heatmap(pearsoncorr, \n",
    "            xticklabels=pearsoncorr.columns,\n",
    "            yticklabels=pearsoncorr.columns,\n",
    "            cmap='RdBu_r',\n",
    "            annot=True,\n",
    "            linewidth=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop correlated features w/ score over 0.9 - retain \"MINS\", \"3P MADE\",\"FTM\",\"REB\"\n",
    "#selected_features = data_reader.select_feature_by_correlation(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Standard Scaling** (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardise scaling of all feature values\n",
    "df_train_selected  = df_train[selected_features]\n",
    "\n",
    "#Apply scaler\n",
    "df_cleaned = df_train_selected.copy()\n",
    "target = df_cleaned.pop('TARGET_5Yrs')\n",
    "\n",
    "df_train_scaled = data_reader.scale_features_by_standard_scaler(df_cleaned)\n",
    "df_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = data_reader.polynomialize_data(X_train, 2)\n",
    "X_val_poly = data_reader.polynomialize_data(X_val, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset using common function data_reader.splitdata\n",
    "X_train, X_val, y_train, y_val  = data_reader.split_data(df_train)\n",
    "#X_train, X_val, y_train, y_val  = data_reader.split_data(df_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class imbalance pipeline\n",
    "\n",
    "!pip install imblearn\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(algorithm):\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('over', SMOTE(sampling_strategy=0.5)),\n",
    "        ('under', RandomUnderSampler(sampling_strategy=0.75)),\n",
    "        ('model', algorithm)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import Trials, STATUS_OK, tpe, hp, fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(space):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    #define algorithm\n",
    "    \n",
    "    xgboost = xgb.XGBClassifier(\n",
    "        max_depth = int(space['max_depth']),\n",
    "        learning_rate = space['learning_rate'],\n",
    "        min_child_weight = space['min_child_weight'],\n",
    "        gamma = space['gamma'],\n",
    "        subsample = space['subsample'],\n",
    "        colsample_bytree = space['colsample_bytree'],\n",
    "        #scale_pos_weight = space['scale_pos_weight'],\n",
    "        use_label_encoder=False,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # CV of pipeline including that algorithm \n",
    "    acc = cross_val_score(model_pipeline(xgboost), X_train, y_train, cv=5, scoring=\"roc_auc\").mean()\n",
    "\n",
    "    return{'loss': 1-acc, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define search space\n",
    "\n",
    "space = {\n",
    "    'max_depth' : hp.choice('max_depth', range(5, 20, 1)),\n",
    "    'learning_rate' : hp.quniform('learning_rate', 0.01, 0.5, 0.05),\n",
    "    'min_child_weight' : hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'gamma' : hp.quniform('gamma', 0, 0.1, 0.01),\n",
    "    'subsample' : hp.quniform('subsample', 0.1, 1, 0.05),\n",
    "    'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1.0, 0.05)\n",
    "    #'scale_pos_weight' : hp.quniform('scale_pos_weight', 1, class_weight, class_weight / 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal parameters \n",
    "\n",
    "best = fmin(\n",
    "    fn=objective,   \n",
    "    space=space,       \n",
    "    algo=tpe.suggest,       \n",
    "    max_evals=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: \", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model using the best hyperparamters from the hyperparameter search.\n",
    "\n",
    "xgboost_2 = xgb.XGBClassifier(\n",
    "    max_depth = best['max_depth'],\n",
    "    learning_rate = best['learning_rate'],\n",
    "    min_child_weight = best['min_child_weight'],\n",
    "    gamma = best['gamma'],    \n",
    "    subsample = best['subsample'],\n",
    "    colsample_bytree = best['colsample_bytree'],\n",
    "    #scale_pos_weight = best['scale_pos_weight'],\n",
    "    use_label_encoder=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "best_model = model_pipeline(xgboost_2)\n",
    "\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Selection and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use GridSearchCV to optimise parameters\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "  \n",
    "# defining parameter range \n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf']}  \n",
    "  \n",
    "grid = GridSearchCV(SVC(probability=True), param_grid, refit = True, verbose = 3, scoring=\"roc_auc\", n_jobs=-2) \n",
    "  \n",
    "# fitting the model for grid search \n",
    "grid.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the optimised parameters\n",
    "print(grid.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create model with the optimised parameters\n",
    "model = SVC(C=100, break_ties=False, class_weight='balanced', coef0=0.0,\n",
    "                           decision_function_shape='ovr', degree=3,\n",
    "                           gamma=0.0001, kernel='rbf', max_iter=-1,\n",
    "                           probability=True, random_state=None, shrinking=True,\n",
    "                           tol=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store model in /models\n",
    "from joblib import dump\n",
    "dump(model, '../models/' + experiment_label + '.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create predictions for train and validation\n",
    "y_train_preds = model.predict(X_train)\n",
    "y_val_preds = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate train predictions\n",
    "#from src.models.aj_metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import classification_report\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.models.aj_metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training performance results\n",
    "print(\"ROC AUC Score:\")\n",
    "print(roc_auc_score(y_train,y_train_preds))\n",
    "print(classification_report(y_train, y_train_preds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "print(confusion_matrix(y_train, y_train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "plot_roc_curve(model,X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision Recall Curve\n",
    "plot_precision_recall_curve(model,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation performance analysis\n",
    "print(\"ROC AUC Score:\")\n",
    "print(roc_auc_score(y_val,y_val_preds))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(classification_report(y_val, y_val_preds)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix\n",
    "print(confusion_matrix(y_train, y_train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve\n",
    "plot_roc_curve(model,X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision Recall Curve\n",
    "plot_precision_recall_curve(model,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output predictions\n",
    "X_test = df_test\n",
    "y_test_preds = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id': range(0,3799), 'TARGET_5Yrs': [p for p in y_test_preds]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"../reports/\" + user_label + \"_\" + experiment_label + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outcome**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After outputting the predictions into kaggle, the final score was 0.00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
